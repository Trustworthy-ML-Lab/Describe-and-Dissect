{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd0d6218",
   "metadata": {},
   "source": [
    "## Ablation: CLIP Caption Quantitative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acbd835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import clip\n",
    "import torch\n",
    "import utils\n",
    "import data_utils\n",
    "import DnD_models\n",
    "import scoring_function\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from diffusers import DPMSolverMultistepScheduler\n",
    "\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from evaluate import load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fa43ae",
   "metadata": {},
   "source": [
    "### Experiment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822ac3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_name = 'ViT-B/16'\n",
    "target_name = 'resnet50'\n",
    "target_layer = 'fc'\n",
    "d_probe = 'imagenet_broden'\n",
    "concept_set = '../data/20k.txt'\n",
    "\n",
    "with open(concept_set, 'r') as f: \n",
    "    concepts = (f.read()).split('\\n')\n",
    "\n",
    "batch_size = 200\n",
    "device = 'cuda'\n",
    "pool_mode = 'avg'\n",
    "\n",
    "results_dir = 'exp_results'\n",
    "saved_acts_dir = 'saved_activations'\n",
    "num_images_to_check = 10\n",
    "blip_batch_size = 10\n",
    "tag = \"clip_fc\"\n",
    "\n",
    "bertscore = load(\"bertscore\")\n",
    "ids_to_check = list(range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e2e609",
   "metadata": {},
   "source": [
    "### Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c780322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define embedding models\n",
    "\n",
    "mpnetmodel = SentenceTransformer('all-mpnet-base-v2')\n",
    "clip_model, clip_preprocess = clip.load(clip_name, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d79d665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Stable Diffusion\n",
    "\n",
    "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.safety_checker = lambda images, **kwargs: (images, [False] * len(images))\n",
    "generator = torch.Generator(device=device).manual_seed(0)\n",
    "pipe = pipe.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea5ab22",
   "metadata": {},
   "source": [
    "### Set Up Results File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc403f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up file/directory paths for saving results\n",
    "pot_column_names = ['Neuron ID'] + ['Concept {}'.format(i) for i in range(5)]\n",
    "all_concepts = pd.DataFrame(columns=pot_column_names)\n",
    "result_column_names = ['Neuron ID', 'Label 1', 'Label 2', 'Label 3']\n",
    "final_concepts = pd.DataFrame(columns=result_column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46bf68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results folder\n",
    "results_path = utils.create_layer_folder(results_dir = results_dir, base_dir = \".\", target_name = target_name, \n",
    "                          d_probe = d_probe, layer = target_layer, tag = tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60c35f1",
   "metadata": {},
   "source": [
    "### Construct Augmented Probing Data (DnD Step 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e595f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get activations\n",
    "target_save_name = utils.get_save_names(target_name = target_name,\n",
    "                                  target_layer = target_layer, d_probe = d_probe,\n",
    "                                  pool_mode=pool_mode, base_dir = '.', saved_acts_dir = saved_acts_dir)\n",
    "\n",
    "utils.save_activations(target_name = target_name, target_layers = [target_layer],\n",
    "                       d_probe = d_probe, batch_size = batch_size, device = device,\n",
    "                       pool_mode=pool_mode, base_dir = '.', saved_acts_dir = saved_acts_dir)\n",
    "\n",
    "target_feats = torch.load(target_save_name, map_location='cpu')\n",
    "\n",
    "pil_data = data_utils.get_data(d_probe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e38fd21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find top activating images\n",
    "top_vals, top_ids = torch.topk(target_feats, k=num_images_to_check, dim=0)\n",
    "\n",
    "all_imgs = []\n",
    "all_img_ids = {neuron_id:[] for neuron_id in ids_to_check}\n",
    "\n",
    "# Find top activating image crops\n",
    "for t, orig_id in enumerate(ids_to_check):\n",
    "    print(\"Cropping for Neuron {}/{}\".format(t+1,len(ids_to_check)))\n",
    "    \n",
    "    activating_images = []\n",
    "    for i, top_id in enumerate(top_ids[:, orig_id]):\n",
    "        \n",
    "        # Reshape activating images\n",
    "        im, label = pil_data[top_id]\n",
    "        im = im.resize([375,375])\n",
    "        \n",
    "        # Add image to d_probe with image ID\n",
    "        all_img_ids[orig_id].append(len(all_imgs))\n",
    "        all_imgs.append(im)\n",
    "        activating_images.append(im)\n",
    "        \n",
    "    cropped_images = []\n",
    "    \n",
    "    # Get crops - FC layers do not have feature maps\n",
    "    if(target_layer != 'fc'):\n",
    "        cropped_images = DnD_models.get_attention_crops(target_name, activating_images, orig_id, num_crops_per_image = 4, target_layers = [target_layer], device = device)\n",
    "\n",
    "    # Add crops into d_probe with image ID\n",
    "    for img in cropped_images:\n",
    "        all_img_ids[orig_id].append(len(all_imgs))\n",
    "        all_imgs.append(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1871dcad",
   "metadata": {},
   "source": [
    "### Captioning with CLIP (DnD Step 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99c3a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get target activations with D_probe + D_cropped\n",
    "target_feats = utils.get_target_activations(target_name, all_imgs, [target_layer])\n",
    "\n",
    "# Find top activating images\n",
    "top_vals, top_ids = torch.sort(target_feats, dim=0, descending = True)\n",
    "comp_words = {orig_id : [] for orig_id in ids_to_check}\n",
    "top_images = {orig_id:[] for orig_id in ids_to_check}\n",
    "\n",
    "# Step 2 - Generate Candidate Concepts\n",
    "for neuron_num, orig_id in enumerate(ids_to_check):\n",
    "    print(\"Neuron: {} ({}/{})\".format(orig_id, neuron_num+1, len(ids_to_check)))\n",
    "\n",
    "    # Plot and save highest activating images\n",
    "    fig, images, top_images = utils.get_top_images(orig_id, top_ids, top_images, \n",
    "                                                   all_imgs, all_img_ids, num_images_to_check, \n",
    "                                                   blip_batch_size)\n",
    "    utils.save_activating_fig(fig, results_path, orig_id)\n",
    "    \n",
    "    text = clip.tokenize([\"{}\".format(word) for word in concepts]).to(device)\n",
    "    concept_embeds = utils.get_clip_text_features(clip_model, text)\n",
    "    concept_embeds /= concept_embeds.norm(dim=-1, keepdim = True)\n",
    "    \n",
    "    image_embeds = utils.get_clip_image_features(clip_model, clip_preprocess, images, device = device)\n",
    "    image_embeds /= image_embeds.norm(dim=-1, keepdim = True)\n",
    "    \n",
    "    clip_feats = image_embeds @ concept_embeds.T\n",
    "    \n",
    "    top_words_idx = torch.argmax(clip_feats,1)\n",
    "    descriptions = [concepts[idx] for idx in top_words_idx]\n",
    "    for t, label in enumerate(descriptions):\n",
    "        print(\"Image {}: {}\".format(t,label))\n",
    "\n",
    "    # Summarize CLIP descriptions\n",
    "    for i in range(5):\n",
    "        cand_concept = DnD_models.GPT_model_single(descriptions)\n",
    "        comp_words[orig_id].append(cand_concept)\n",
    "        random.shuffle(descriptions)\n",
    "        print(\"Candidate Concept {}: {}\".format(i+1, cand_concept))\n",
    "    all_concepts.loc[len(all_concepts)] = [orig_id] + comp_words[orig_id]\n",
    "\n",
    "# Save candidate concepts\n",
    "utils.save_potential_concepts(all_concepts, results_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6beee847",
   "metadata": {},
   "source": [
    "### Best Concept Selection (DnD Step 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e0cf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We adjust concepts with certain vague words to help SD generation\n",
    "\"\"\"\n",
    "\n",
    "replace_set = ['design','designs','graphic','graphics']\n",
    "for orig_id in ids_to_check:\n",
    "    comp_words[orig_id] = [concept.lower() for concept in comp_words[orig_id]]\n",
    "    for i, word in enumerate(comp_words[orig_id]):\n",
    "        if word[-1] == '.':\n",
    "            comp_words[orig_id][i] = word[:-1]\n",
    "        if word.split()[-1] in replace_set:\n",
    "            new_concept = word + ' background'\n",
    "            comp_words[orig_id].append(new_concept)\n",
    "    comp_words[orig_id] = list(set(comp_words[orig_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52530e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "pil_data = data_utils.get_data(d_probe)\n",
    "d_probe_len = len(pil_data)\n",
    "all_final_results = {neuron_id : [] for neuron_id in ids_to_check}\n",
    "\n",
    "num_images_per_prompt = 10\n",
    "top_K_param = 10\n",
    "beta_images_param = 5\n",
    "scoring_func = 'topk-sq-mean'\n",
    "\n",
    "sd_prompt = 'One realistic image of {}'\n",
    "num_inference_steps = 50\n",
    "\n",
    "# Step 3 Main Code\n",
    "for list_id, orig_id in enumerate(ids_to_check):\n",
    "\n",
    "    # Initialize starting concepts\n",
    "    word_list = comp_words[orig_id]\n",
    "    \n",
    "    print(\"Neuron {}\".format(orig_id))\n",
    "\n",
    "    # Account for modified candidate concept (if necessary)\n",
    "    labels_to_check = len(word_list)\n",
    "\n",
    "    print(\"# Labels to Check: {}\".format(labels_to_check), \"   # Images per Concept: {}\".format(num_images_per_prompt))\n",
    "\n",
    "    add_im = {}\n",
    "    add_im_id = {}\n",
    "    all_sd_imgs = []\n",
    "\n",
    "    # Generate images for each label\n",
    "    for label_id in range(labels_to_check):\n",
    "        \n",
    "        print(\"Label {}/{}: {}\".format(label_id + 1, labels_to_check, word_list[label_id]))\n",
    "        pred_label = sd_prompt.format(word_list[label_id])\n",
    "        add_im_id[label_id] = []\n",
    "        \n",
    "        add_im, add_im_id, all_sd_imgs = DnD_models.generate_sd_images(add_im, add_im_id, all_sd_imgs, \n",
    "                                                                  pred_label, label_id, pipe, generator,\n",
    "                                                                  num_images_per_prompt, num_inference_steps)\n",
    "    \n",
    "    # Concept Scoring\n",
    "    target_feats = utils.get_target_activations(target_name, all_sd_imgs, [target_layer])\n",
    "    \n",
    "    ranks, highest_activating = utils.rank_images(target_feats, orig_id, labels_to_check,\n",
    "                                                 add_im_id, add_im, top_K_param)\n",
    "    \n",
    "    clip_weight = scoring_function.compare_images(top_images[orig_id], highest_activating, clip_name, \n",
    "                                                  device, target_name, top_K_param)\n",
    "    \n",
    "    top_avg_topk = scoring_function.get_score(ranks, mode = scoring_func, hyp_param = beta_images_param)\n",
    "    \n",
    "    top_avg_comb = []\n",
    "    for i in range(len(clip_weight)):\n",
    "        concept_rank = len(top_avg_topk) - scoring_function.find_by_last(top_avg_topk, clip_weight[i][1])\n",
    "        weight = clip_weight[i][0]\n",
    "        concept_score = concept_rank * weight\n",
    "        top_avg_comb.append((concept_score, clip_weight[i][1]))\n",
    "        \n",
    "    top_avg_comb.sort(reverse = True)\n",
    "    \n",
    "    # Save results in .csv file\n",
    "    for label_num in range(3):\n",
    "        if(label_num < len(top_avg_comb)):\n",
    "            all_final_results[orig_id] += [word_list[top_avg_comb[label_num][1]]]\n",
    "        else:\n",
    "            all_final_results[orig_id] += [' ']\n",
    "    final_concepts.loc[len(final_concepts)] = [orig_id] + all_final_results[orig_id]\n",
    "    \n",
    "    # Print results\n",
    "    print('------------------------------\\n')\n",
    "    print('Neuron {}:'.format(orig_id))\n",
    "    for k, word in enumerate(all_final_results[orig_id]):\n",
    "        if(word != \" \"):\n",
    "            print(\"Label {}: {}\".format(k + 1, word))\n",
    "        else:\n",
    "            break\n",
    "    print('\\n------------------------------')\n",
    "    \n",
    "utils.save_final_results(final_concepts, results_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d498e3fa",
   "metadata": {},
   "source": [
    "### Find FC Layer Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fb7a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ground truths\n",
    "with open('../data/imagenet_labels.txt', 'r') as f:\n",
    "    classes = f.read().split('\\n')\n",
    "\n",
    "# Get DnD labels\n",
    "dnd_labels = pd.read_csv('./exp_results/resnet50_imagenet_broden_layerfc_results_clip_fc/all_result_concepts/final_results.csv')\n",
    "dnd_preds = list(dnd_labels['Label 1'])\n",
    "\n",
    "# Get DnD labels with CLIP captioning\n",
    "dnd_clip_preds = []\n",
    "for orig_id in ids_to_check:\n",
    "    dnd_clip_preds.append(all_final_results[orig_id][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ca927f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate similarities between labels and ground truths\n",
    "\n",
    "clip_cos, mpnet_cos = utils.get_cos_similarity(dnd_preds, classes, clip_model, mpnetmodel, device, batch_size)\n",
    "bert_score = bertscore.compute(predictions=dnd_preds, references=classes, lang=\"en\")\n",
    "print(\"DnD w/ BLIP - Clip similarity: {:.4f}, mpnet similarity: {:.4f}, BERTScore: {:4f}\".format(clip_cos, mpnet_cos, sum(bert_score[\"f1\"]) / len(bert_score[\"f1\"])))\n",
    "clip_cos, mpnet_cos = utils.get_cos_similarity(dnd_clip_preds, classes, clip_model, mpnetmodel, device, batch_size)\n",
    "bert_score = bertscore.compute(predictions=dnd_clip_preds, references=classes, lang=\"en\")\n",
    "print(\"DnD w/ CLIP - Clip similarity: {:.4f}, mpnet similarity: {:.4f}, BERTScore: {:4f}\".format(clip_cos, mpnet_cos, sum(bert_score[\"f1\"]) / len(bert_score[\"f1\"])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

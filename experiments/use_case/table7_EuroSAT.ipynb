{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fee4835d-4ad5-4ff7-824b-339014245a6e",
   "metadata": {},
   "source": [
    "## Use Case: Spurious Correlations in EuroSAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b090838-2954-458f-a276-4f6d5b883cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchsat.models import resnet50\n",
    "import torchsat.transforms.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torchsat.transforms.transforms_cls as T_cls\n",
    "from torchsat.datasets.folder import ImageFolder\n",
    "from torchsat.models.utils import get_model\n",
    "\n",
    "from evaluate import load\n",
    "\n",
    "from collections import Counter\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "os.chdir(\"sandbox-DnD\")\n",
    "import clip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355d77da-1925-4524-9318-eee863344b02",
   "metadata": {},
   "source": [
    "### Train ResNet50 model on EuroSAT dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6c27be6-7eee-421f-8a9c-a99117540e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: For replication purposes, our used .pth file is provided in the GitHub, so DO NOT run this cell to replicate our results\n",
    "\n",
    "'''\n",
    "!mkdir output\n",
    "!python scripts/train_cls.py \\\n",
    "         --train-path EuroSAT/train \\\n",
    "         --val-path EuroSAT/val/ \\\n",
    "         --model resnet50 \\\n",
    "         --num-classes 10 \\\n",
    "         --device cuda \\\n",
    "         -b 64 \\\n",
    "         --print-freq 20 \\\n",
    "         --ckp-di output\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79825421-9f8a-4278-980e-8260f810e194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_to_idx = {\n",
    "        'AnnualCrop': 0,\n",
    "        'Forest': 1,\n",
    "        'HerbaceousVegetation': 2,\n",
    "        'Highway': 3,\n",
    "        'Industrial': 4,\n",
    "        'Pasture': 5,\n",
    "        'PermanentCrop': 6,\n",
    "        'Residential': 7,\n",
    "        'River': 8,\n",
    "        'SeaLake': 9,\n",
    "    }\n",
    "idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
    "\n",
    "# load trained model\n",
    "model_sat = resnet50(num_classes=10)\n",
    "ckp = 'output/cls_epoch_35.pth'\n",
    "model_sat.load_state_dict(torch.load(ckp, map_location=torch.device('cuda')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1da600e-496e-4e54-a48d-4ccbe54adc3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1552, Accuracy: 2572/2700 (95%)\n",
      "\n",
      "\n",
      "AnnualCrop: Accuracy: 285/300 (95%)\n",
      "\n",
      "\n",
      "Forest: Accuracy: 294/300 (98%)\n",
      "\n",
      "\n",
      "HerbaceousVegetation: Accuracy: 281/300 (94%)\n",
      "\n",
      "\n",
      "Highway: Accuracy: 240/250 (96%)\n",
      "\n",
      "\n",
      "Industrial: Accuracy: 242/250 (97%)\n",
      "\n",
      "\n",
      "Pasture: Accuracy: 183/200 (92%)\n",
      "\n",
      "\n",
      "PermanentCrop: Accuracy: 227/250 (91%)\n",
      "\n",
      "\n",
      "Residential: Accuracy: 296/300 (99%)\n",
      "\n",
      "\n",
      "River: Accuracy: 232/250 (93%)\n",
      "\n",
      "\n",
      "SeaLake: Accuracy: 292/300 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate(epoch, model, criterion, data_loader, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    class_correct = [0 for _ in range(10)]\n",
    "    class_count = [0 for _ in range(10)]\n",
    "    with torch.no_grad():\n",
    "        for idx, (image, target) in enumerate(data_loader):\n",
    "            image = image.to(device, non_blocking=True)\n",
    "            target = target.to(device, non_blocking=True)\n",
    "            output = model(image)\n",
    "            loss += criterion(output, target).item()\n",
    "\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            \n",
    "            for i in range(len(target)):\n",
    "                label = target[i].item()\n",
    "                prediction = pred[i].item()\n",
    "                if prediction == label:\n",
    "                    class_correct[label] += 1\n",
    "                class_count[label] += 1\n",
    "\n",
    "        loss /= len(data_loader.dataset)/data_loader.batch_size\n",
    "\n",
    "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            loss, correct, len(data_loader.dataset),\n",
    "            100. * correct / len(data_loader.dataset)))\n",
    "        \n",
    "        class_names = [\n",
    "            'AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway',\n",
    "            'Industrial', 'Pasture', 'PermanentCrop', 'Residential',\n",
    "            'River', 'SeaLake'\n",
    "        ]\n",
    "\n",
    "        for i in range(10):\n",
    "            if class_count[i] > 0:\n",
    "                accuracy = 100. * class_correct[i] / class_count[i]\n",
    "                print('\\n{}: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "                    class_names[i], class_correct[i], class_count[i], accuracy))\n",
    "            else:\n",
    "                print('\\n{}: No samples found\\n'.format(class_names[i]))\n",
    "\n",
    "\n",
    "val_transform = T_cls.Compose([\n",
    "        T_cls.ToTensor(),\n",
    "        T_cls.Normalize(),\n",
    "    ])\n",
    "dataset_val = ImageFolder(\"EuroSAT/val/\", val_transform)\n",
    "\n",
    "# Evaluate on original model\n",
    "\n",
    "evaluate(35, model_sat, nn.CrossEntropyLoss(), DataLoader(dataset_val, batch_size=16, shuffle=True), torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48efcdb4-eff5-4450-b1fe-6d4c78a088f6",
   "metadata": {},
   "source": [
    "### Load Data and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb7b597d-d2fd-4ebc-bf9c-8823666cb5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define embedding models\n",
    "\n",
    "mpnetmodel = SentenceTransformer('all-mpnet-base-v2')\n",
    "clip_model, _ = clip.load('ViT-B/16', device='cuda')\n",
    "bertscore = load(\"bertscore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7cd7e87-69ec-420e-983e-a39cbe50b364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DnD labels\n",
    "\n",
    "csv = pd.read_csv(\"data/DnD_results/eurosat_results/layer4.csv\")\n",
    "labels = list(csv[\"Label 1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70d5888-20b0-47fe-a5c8-fb50c10c5341",
   "metadata": {},
   "source": [
    "### Investigate most common concept in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7dbf241-eb73-4bd6-8b56-0906e556f1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most common concept is 'fishing' with 883 occurrences.\n"
     ]
    }
   ],
   "source": [
    "def most_common_concept(strings):\n",
    "    # Tokenize and normalize\n",
    "    tokens = []\n",
    "    for str in strings:\n",
    "        words = re.findall(r'\\w+', str.lower())\n",
    "        tokens.extend(words)\n",
    "    \n",
    "    # Count the frequency of each token\n",
    "    token_counts = Counter(tokens)\n",
    "    \n",
    "    # Return the most common token\n",
    "    return token_counts.most_common(1)[0]\n",
    "\n",
    "common_concept = most_common_concept(labels)\n",
    "print(f\"The most common concept is '{common_concept[0]}' with {common_concept[1]} occurrences.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6315641-7269-441a-836a-a1db4f000d54",
   "metadata": {},
   "source": [
    "### Define Pruning Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c564b90-0fc7-4693-90c0-acfd50130195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_neurons(ids_to_prune):\n",
    "    layer = \"layer4\"\n",
    "    num_neurons = eval(\"model_sat.{}[-1].conv3.out_channels\".format(layer))\n",
    "    \n",
    "    for block in eval(\"model_sat.{}\".format(layer)):\n",
    "        for comp in block.children():\n",
    "            if (hasattr(comp, \"out_channels\") and comp.out_channels == num_neurons) or (hasattr(comp, \"num_features\") and comp.num_features == num_neurons):\n",
    "                if comp.bias != None:\n",
    "                    mask_bias = torch.ones_like(comp.bias)\n",
    "                    for id in ids_to_prune:\n",
    "                        mask_bias[id] = 0\n",
    "                    comp = torch.nn.utils.prune.custom_from_mask(comp, \"bias\", mask_bias)\n",
    "                mask_weight = torch.ones_like(comp.weight)\n",
    "                for id in ids_to_prune:\n",
    "                    mask_weight[id] = 0\n",
    "                comp = torch.nn.utils.prune.custom_from_mask(comp, \"weight\", mask_weight)\n",
    "            if hasattr(comp, \"__iter__\"):\n",
    "                for ds_comp in comp:\n",
    "                    if ds_comp.bias != None:\n",
    "                        mask_bias = torch.ones_like(ds_comp.bias)\n",
    "                        for id in ids_to_prune:\n",
    "                            mask_bias[id] = 0\n",
    "                        ds_comp = torch.nn.utils.prune.custom_from_mask(ds_comp, \"bias\", mask_bias)\n",
    "                    mask_weight = torch.ones_like(ds_comp.weight)\n",
    "                    for id in ids_to_prune:\n",
    "                        mask_weight[id] = 0\n",
    "                    ds_comp = torch.nn.utils.prune.custom_from_mask(ds_comp, \"weight\", mask_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be407dff-e5dd-4242-bcf2-9c9d0389b9e4",
   "metadata": {},
   "source": [
    "### Prune \"fishing\" neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c72443c-fe6d-4338-b93f-399e15d38d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1563, Accuracy: 2572/2700 (95%)\n",
      "\n",
      "\n",
      "AnnualCrop: Accuracy: 285/300 (95%)\n",
      "\n",
      "\n",
      "Forest: Accuracy: 294/300 (98%)\n",
      "\n",
      "\n",
      "HerbaceousVegetation: Accuracy: 281/300 (94%)\n",
      "\n",
      "\n",
      "Highway: Accuracy: 240/250 (96%)\n",
      "\n",
      "\n",
      "Industrial: Accuracy: 242/250 (97%)\n",
      "\n",
      "\n",
      "Pasture: Accuracy: 183/200 (92%)\n",
      "\n",
      "\n",
      "PermanentCrop: Accuracy: 227/250 (91%)\n",
      "\n",
      "\n",
      "Residential: Accuracy: 296/300 (99%)\n",
      "\n",
      "\n",
      "River: Accuracy: 232/250 (93%)\n",
      "\n",
      "\n",
      "SeaLake: Accuracy: 292/300 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words = [\"fishing\"]\n",
    "\n",
    "ids_to_prune = []\n",
    "for idx in range(len(labels)):\n",
    "    prune = False\n",
    "    for word in words:\n",
    "        if word in labels[idx].lower():\n",
    "            prune = True\n",
    "    if prune:\n",
    "        ids_to_prune.append(idx)\n",
    "\n",
    "prune_neurons(ids_to_prune)\n",
    "\n",
    "evaluate(35, model_sat, nn.CrossEntropyLoss(), DataLoader(dataset_val, batch_size=16, shuffle=True), torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3096a0-463b-4b13-8219-7b46272a2237",
   "metadata": {},
   "source": [
    "### Prune \"purple\" and \"pink\" neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58deb270-0552-45a4-8213-5d2d20c18baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 9.5645, Accuracy: 657/2700 (24%)\n",
      "\n",
      "\n",
      "AnnualCrop: Accuracy: 265/300 (88%)\n",
      "\n",
      "\n",
      "Forest: Accuracy: 0/300 (0%)\n",
      "\n",
      "\n",
      "HerbaceousVegetation: Accuracy: 0/300 (0%)\n",
      "\n",
      "\n",
      "Highway: Accuracy: 155/250 (62%)\n",
      "\n",
      "\n",
      "Industrial: Accuracy: 0/250 (0%)\n",
      "\n",
      "\n",
      "Pasture: Accuracy: 0/200 (0%)\n",
      "\n",
      "\n",
      "PermanentCrop: Accuracy: 47/250 (19%)\n",
      "\n",
      "\n",
      "Residential: Accuracy: 0/300 (0%)\n",
      "\n",
      "\n",
      "River: Accuracy: 190/250 (76%)\n",
      "\n",
      "\n",
      "SeaLake: Accuracy: 0/300 (0%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reload model\n",
    "model_sat = resnet50(num_classes=10)\n",
    "ckp = 'output/cls_epoch_35.pth'\n",
    "model_sat.load_state_dict(torch.load(ckp, map_location=torch.device('cuda')))\n",
    "\n",
    "words = [\"purple\", \"pink\"]\n",
    "\n",
    "ids_to_prune = []\n",
    "for idx in range(len(labels)):\n",
    "    prune = False\n",
    "    for word in words:\n",
    "        if word in labels[idx].lower():\n",
    "            prune = True\n",
    "    if prune:\n",
    "        ids_to_prune.append(idx)\n",
    "\n",
    "prune_neurons(ids_to_prune)\n",
    "\n",
    "evaluate(35, model_sat, nn.CrossEntropyLoss(), DataLoader(dataset_val, batch_size=16, shuffle=True), torch.device('cuda'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-kernel",
   "language": "python",
   "name": "jupyter-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

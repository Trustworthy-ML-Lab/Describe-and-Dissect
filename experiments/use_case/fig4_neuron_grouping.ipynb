{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f696a288-eb3c-4c49-b6f0-11c7037a8027",
   "metadata": {},
   "source": [
    "## Use Case: Locating Conceptual Groupings in T2V-ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0191afcc-2a5e-4fcf-8832-6459c53c45b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "import clip\n",
    "import torch\n",
    "import utils\n",
    "import data_utils\n",
    "import DnD_models\n",
    "import scoring_function\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "from time import time\n",
    "import math\n",
    "\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from diffusers import DPMSolverMultistepScheduler\n",
    "\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a97b18-9221-4342-9130-7bcd3843c112",
   "metadata": {},
   "source": [
    "### Experiment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9d0edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_name = 'ViT-B/16'\n",
    "target_name = 'resnet18_tile2vec'\n",
    "target_layer = 'layer1'\n",
    "d_probe = 'NAIP'\n",
    "\n",
    "batch_size = 200\n",
    "device = 'cuda:0'\n",
    "pool_mode = 'avg'\n",
    "\n",
    "results_dir = 'exp_results'\n",
    "saved_acts_dir = 'saved_activations'\n",
    "num_images_to_check = 10\n",
    "blip_batch_size = 10\n",
    "\n",
    "tag = \"concept_grouping\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca2faad-b488-4b69-91b8-f151af509acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dims = {\"layer1\" : 64, \"layer2\" : 128, \"layer3\" : 256, \"layer4\" : 512, \"layer5\" : 512}\n",
    "\n",
    "ids_to_check = list(range(layer_dims[target_layer]))\n",
    "print(ids_to_check)\n",
    "print(len(ids_to_check))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bc0f4d-7c46-4f82-9aa5-9c89efdba4a9",
   "metadata": {},
   "source": [
    "### Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df20d6f-fbce-4ccb-a9be-68aa66659f6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load BLIP model\n",
    "\n",
    "BLIP_PATH = \"/expanse/lustre/scratch/nbai/temp_project/model_base_capfilt_large.pth\"\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device) \n",
    "pretrained_dict = torch.load(BLIP_PATH)\n",
    "model_dict = model.state_dict()\n",
    "pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "model_dict.update(pretrained_dict)\n",
    "model.load_state_dict(model_dict)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbac2542-995c-45dc-978b-6e81f64b132a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Stable Diffusion\n",
    "\n",
    "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.safety_checker = lambda images, **kwargs: (images, [False] * len(images))\n",
    "generator = torch.Generator(device=device).manual_seed(0)\n",
    "pipe = pipe.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2440e4ea-3622-4f83-9b45-6e2434f2f513",
   "metadata": {},
   "source": [
    "### Set Up Results File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd61627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up file/directory paths for saving results\n",
    "pot_column_names = ['Neuron ID'] + ['Concept {}'.format(i) for i in range(5)]\n",
    "all_concepts = pd.DataFrame(columns=pot_column_names)\n",
    "result_column_names = ['Neuron ID', 'Label 1', 'Label 2', 'Label 3']\n",
    "final_concepts = pd.DataFrame(columns=result_column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d582d4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results folder\n",
    "results_path = utils.create_layer_folder(results_dir = results_dir, base_dir = \".\", target_name = target_name, \n",
    "                          d_probe = d_probe, layer = target_layer, tag = tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36506eab-e6b5-44d7-af43-e706566f5c24",
   "metadata": {},
   "source": [
    "### Construct Augmented Probing Data (DnD Step 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9723898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get activations\n",
    "target_save_name = utils.get_save_names(target_name = target_name,\n",
    "                                  target_layer = target_layer, d_probe = d_probe,\n",
    "                                  pool_mode=pool_mode, base_dir = '.', saved_acts_dir = saved_acts_dir)\n",
    "\n",
    "utils.save_activations(target_name = target_name, target_layers = [target_layer],\n",
    "                       d_probe = d_probe, batch_size = batch_size, device = device,\n",
    "                       pool_mode=pool_mode, base_dir = '.', saved_acts_dir = saved_acts_dir)\n",
    "\n",
    "target_feats = torch.load(target_save_name, map_location='cpu')\n",
    "\n",
    "pil_data = data_utils.get_data(d_probe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80850306",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find top activating images\n",
    "top_vals, top_ids = torch.topk(target_feats, k=num_images_to_check, dim=0)\n",
    "\n",
    "all_imgs = []\n",
    "all_img_ids = {neuron_id:[] for neuron_id in ids_to_check}\n",
    "\n",
    "# Find top activating image crops\n",
    "for t, orig_id in enumerate(ids_to_check):\n",
    "    print(\"Cropping for Neuron {}/{}\".format(t+1,len(ids_to_check)))\n",
    "    activating_images = []\n",
    "    for i, top_id in enumerate(top_ids[:, orig_id]):\n",
    "        im = pil_data[top_id][0].type(torch.FloatTensor)\n",
    "        all_img_ids[orig_id].append(len(all_imgs))\n",
    "        all_imgs.append(im)\n",
    "        activating_images.append(im)\n",
    "    cropped_images = []\n",
    "    if(target_layer != 'fc'):\n",
    "        cropped_images = DnD_models.get_attention_crops(target_name, activating_images, orig_id, num_crops_per_image = 4, target_layers = [target_layer], device = device)\n",
    "\n",
    "    for img in cropped_images:\n",
    "        all_img_ids[orig_id].append(len(all_imgs))\n",
    "        all_imgs.append(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3646464d-3806-40e0-9aa3-de30d1b3f8be",
   "metadata": {},
   "source": [
    "### Generative Captioning (DnD Step 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63779cd4-d6cd-4e10-82a7-f6a696ba0eba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get target activations with D_probe + D_cropped\n",
    "target_feats = utils.get_target_activations(target_name, all_imgs, [target_layer])\n",
    "\n",
    "# Find top activating images\n",
    "top_vals, top_ids = torch.sort(target_feats, dim=0, descending = True)\n",
    "comp_words = {orig_id : [] for orig_id in ids_to_check}\n",
    "top_images = {orig_id:[] for orig_id in ids_to_check}\n",
    "\n",
    "# Step 2 - Generate Candidate Concepts\n",
    "for neuron_num, orig_id in enumerate(ids_to_check):\n",
    "    print(\"Neuron: {} ({}/{})\".format(orig_id, neuron_num+1, len(ids_to_check)))\n",
    "\n",
    "    # Plot and save highest activating images\n",
    "    fig, images, top_images = utils.get_top_images(orig_id, top_ids, top_images, \n",
    "                                                   all_imgs, all_img_ids, num_images_to_check, \n",
    "                                                   blip_batch_size, convert_from_np = True)\n",
    "    utils.save_activating_fig(fig, results_path, orig_id)\n",
    "    \n",
    "    # Generate and simplify BLIP Captions\n",
    "    descriptions = DnD_models.blip_caption(model, processor, images, blip_batch_size, device)\n",
    "    for i, description in enumerate(descriptions):\n",
    "        descriptions[i] = DnD_models.GPT_simplify(description)\n",
    "\n",
    "    # Summarize BLIP descriptions\n",
    "    for i in range(5):\n",
    "        cand_concept = DnD_models.GPT_model_naip(descriptions)\n",
    "        comp_words[orig_id].append(cand_concept)\n",
    "        random.shuffle(descriptions)\n",
    "        print(\"Candidate Concept {}: {}\".format(i+1, cand_concept))\n",
    "    all_concepts.loc[len(all_concepts)] = [orig_id] + comp_words[orig_id]\n",
    "\n",
    "# Save candidate concepts\n",
    "utils.save_potential_concepts(all_concepts, results_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f2222d-b06f-4877-989c-f336b82c96a0",
   "metadata": {},
   "source": [
    "### Find Neuron Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9b5299-5551-4723-a851-d314ede25462",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_embed(model, labels):\n",
    "\n",
    "    text_features = []\n",
    "    label_feats = clip.tokenize([\"{}\".format(word) for word in labels]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(math.ceil(len(label_feats)/batch_size)):\n",
    "            text_features.append(model.encode_text(label_feats[batch_size*i:batch_size*(i+1)]))\n",
    "\n",
    "    text_features = torch.cat(text_features, dim=0)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    return text_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3ebe77-eacb-4170-85e8-23a3c541211b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "- We skip Step 3: Best Concept Selection because Stable Diffusion cannot generate land coverage specific synthetic images\n",
    "\n",
    "- Optimal thresholding value can change based on generated labels or target layer. \n",
    "  We suggest experimenting with [0.75, 0.80, 0.85, 0.90, 0.95, 0.97]\n",
    "  \n",
    "\"\"\"\n",
    "\n",
    "THRESH = 0.80\n",
    "clip_model, _ = clip.load(clip_name, device=device)\n",
    "\n",
    "cls = {id : [] for id in ids_to_check}\n",
    "t0 = time()\n",
    "\n",
    "for id in ids_to_check:\n",
    "    descrip = []\n",
    "    class_labels = []\n",
    "    for label in comp_words[id]:\n",
    "        descrip.append(label[:label.find('(')])\n",
    "        class_labels.append(label[label.find('(') + 1 : label.find(')')])\n",
    "    cls[id].append((class_labels, descrip, text_embed(clip_model, class_labels), text_embed(clip_model, descrip)))\n",
    "    \n",
    "print(\"Embedding time: {:.3f}\".format(time()-t0))\n",
    "\n",
    "record = {}\n",
    "for id in ids_to_check:\n",
    "    record[\"Class: {}, {}\".format(cls[id][0][0][0], cls[id][0][0][1])] = [id]\n",
    "    record[\"Concept: {}, {}\".format(cls[id][0][1][0], cls[id][0][1][1])] = [id]\n",
    "\n",
    "grouped_cls = [False for _ in range(len(ids_to_check))]\n",
    "grouped_cpt = [False for _ in range(len(ids_to_check))]\n",
    "\n",
    "for i, id1 in enumerate(ids_to_check[:-1]):\n",
    "    for id2 in ids_to_check[i + 1:]:\n",
    "        if torch.mean(cls[id1][0][2] @ cls[id2][0][2].T) > THRESH and grouped_cls[id1] is False:\n",
    "            record[\"Class: {}, {}\".format(cls[id1][0][0][0],cls[id1][0][0][1])].append(id2)\n",
    "            grouped_cls[id2] = True\n",
    "        if torch.mean(cls[id1][0][3] @ cls[id2][0][3].T) > THRESH and grouped_cpt[id1] is False:\n",
    "            record[\"Concept: {}, {}\".format(cls[id1][0][1][0],cls[id1][0][1][1])].append(id2)\n",
    "            grouped_cpt[id2] = True\n",
    "            \n",
    "record = dict(sorted(record.items(), key=lambda item: len(item[1]), reverse = True))\n",
    "\n",
    "cls_classified = []\n",
    "cpt_classified = []\n",
    "for label in record:\n",
    "    if len(record[label]) > 1 and \"Class\" in label:\n",
    "        record[label] = list(set(record[label]))\n",
    "        record[label].sort()\n",
    "        print(\"{}\\n{}\\n\".format(label, record[label]))\n",
    "        cls_classified.extend(record[label])\n",
    "    elif len(record[label]) > 1 and \"Concept\" in label:\n",
    "        record[label] = list(set(record[label]))\n",
    "        record[label].sort()\n",
    "        print(\"{}\\n{}\\n\".format(label, record[label]))\n",
    "        cpt_classified.extend(record[label])\n",
    "\n",
    "non_classified_by_class = list(set(ids_to_check) - set(cls_classified))\n",
    "non_classified_by_class.sort()\n",
    "\n",
    "non_classified_by_concept = list(set(ids_to_check) - set(cpt_classified))\n",
    "non_classified_by_concept.sort()\n",
    "\n",
    "print(\"No classification by class: {} neurons\\n{}\".format(len(non_classified_by_class), non_classified_by_class))\n",
    "print(\"No classification by concept: {} neurons\\n{}\".format(len(non_classified_by_concept), non_classified_by_concept))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-kernel",
   "language": "python",
   "name": "jupyter-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

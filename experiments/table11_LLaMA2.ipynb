{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de642bf9",
   "metadata": {},
   "source": [
    "## Ablation: DnD with LLaMA2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc60367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import torch\n",
    "import utils\n",
    "import data_utils\n",
    "import DnD_models\n",
    "import scoring_function\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from diffusers import DPMSolverMultistepScheduler\n",
    "\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44aa10a2",
   "metadata": {},
   "source": [
    "### Experiment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2418cbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables for dissection\n",
    "\n",
    "clip_name = 'ViT-B/16'\n",
    "target_name = 'resnet50'\n",
    "target_layer = 'layer1'\n",
    "d_probe = 'imagenet_broden'\n",
    "\n",
    "batch_size = 200\n",
    "device = 'cuda'\n",
    "pool_mode = 'avg'\n",
    "\n",
    "results_dir = 'exp_results'\n",
    "saved_acts_dir = 'saved_activations'\n",
    "num_images_to_check = 10\n",
    "blip_batch_size = 10\n",
    "tag = \"llama2\"\n",
    "\n",
    "ids_to_check = random.sample(range(0, 999), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b41fa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", token=LLAMA_TOKEN)\n",
    "llama_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", token=LLAMA_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed548b9e",
   "metadata": {},
   "source": [
    "### Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000e5e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BLIP model\n",
    "\n",
    "BLIP_PATH = \"\"\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device) \n",
    "pretrained_dict = torch.load(BLIP_PATH)\n",
    "model_dict = model.state_dict()\n",
    "pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "model_dict.update(pretrained_dict)\n",
    "model.load_state_dict(model_dict)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34844885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Stable Diffusion\n",
    "\n",
    "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.safety_checker = lambda images, **kwargs: (images, [False] * len(images))\n",
    "generator = torch.Generator(device=device).manual_seed(0)\n",
    "pipe = pipe.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000fcfc6",
   "metadata": {},
   "source": [
    "### Set Up Results File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cef3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up file/directory paths for saving results\n",
    "pot_column_names = ['Neuron ID'] + ['Concept {}'.format(i) for i in range(5)]\n",
    "all_concepts = pd.DataFrame(columns=pot_column_names)\n",
    "result_column_names = ['Neuron ID', 'Label 1', 'Label 2', 'Label 3']\n",
    "final_concepts = pd.DataFrame(columns=result_column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d45e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results folder\n",
    "results_path = utils.create_layer_folder(results_dir = results_dir, base_dir = \".\", target_name = target_name, \n",
    "                          d_probe = d_probe, layer = target_layer, tag = tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1feb1e",
   "metadata": {},
   "source": [
    "### Construct Augmented Probing Data (DnD Step 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110d351f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get activations\n",
    "target_save_name = utils.get_save_names(target_name = target_name,\n",
    "                                  target_layer = target_layer, d_probe = d_probe,\n",
    "                                  pool_mode=pool_mode, base_dir = '.', saved_acts_dir = saved_acts_dir)\n",
    "\n",
    "utils.save_activations(target_name = target_name, target_layers = [target_layer],\n",
    "                       d_probe = d_probe, batch_size = batch_size, device = device,\n",
    "                       pool_mode=pool_mode, base_dir = '.', saved_acts_dir = saved_acts_dir)\n",
    "\n",
    "target_feats = torch.load(target_save_name, map_location='cpu')\n",
    "\n",
    "pil_data = data_utils.get_data(d_probe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26b2df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find top activating images\n",
    "top_vals, top_ids = torch.topk(target_feats, k=num_images_to_check, dim=0)\n",
    "\n",
    "all_imgs = []\n",
    "all_img_ids = {neuron_id:[] for neuron_id in ids_to_check}\n",
    "\n",
    "# Find top activating image crops\n",
    "for t, orig_id in enumerate(ids_to_check):\n",
    "    print(\"Cropping for Neuron {}/{}\".format(t+1,len(ids_to_check)))\n",
    "    \n",
    "    activating_images = []\n",
    "    for i, top_id in enumerate(top_ids[:, orig_id]):\n",
    "        \n",
    "        # Reshape activating images\n",
    "        im, label = pil_data[top_id]\n",
    "        im = im.resize([375,375])\n",
    "        \n",
    "        # Add image to d_probe with image ID\n",
    "        all_img_ids[orig_id].append(len(all_imgs))\n",
    "        all_imgs.append(im)\n",
    "        activating_images.append(im)\n",
    "        \n",
    "    cropped_images = []\n",
    "    \n",
    "    # Get crops - FC layers do not have feature maps\n",
    "    if(target_layer != 'fc'):\n",
    "        cropped_images = DnD_models.get_attention_crops(target_name, activating_images, orig_id, num_crops_per_image = 4, target_layers = [target_layer], device = device)\n",
    "\n",
    "    # Add crops into d_probe with image ID\n",
    "    for img in cropped_images:\n",
    "        all_img_ids[orig_id].append(len(all_imgs))\n",
    "        all_imgs.append(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34567cec",
   "metadata": {},
   "source": [
    "### Generative Captioning (DnD Step 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f0c1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get target activations with D_probe + D_cropped\n",
    "target_feats = utils.get_target_activations(target_name, all_imgs, [target_layer])\n",
    "\n",
    "# Find top activating images\n",
    "top_vals, top_ids = torch.sort(target_feats, dim=0, descending = True)\n",
    "comp_words = {orig_id : [] for orig_id in ids_to_check}\n",
    "top_images = {orig_id:[] for orig_id in ids_to_check}\n",
    "\n",
    "# Step 2 - Generate Candidate Concepts\n",
    "for neuron_num, orig_id in enumerate(ids_to_check):\n",
    "    print(\"Neuron: {} ({}/{})\".format(orig_id, neuron_num+1, len(ids_to_check)))\n",
    "\n",
    "    # Plot and save highest activating images\n",
    "    fig, images, top_images = utils.get_top_images(orig_id, top_ids, top_images, \n",
    "                                                   all_imgs, all_img_ids, num_images_to_check, \n",
    "                                                   blip_batch_size)\n",
    "    utils.save_activating_fig(fig, results_path, orig_id)\n",
    "    \n",
    "    # Generate and simplify BLIP Captions\n",
    "    descriptions = DnD_models.blip_caption(model, processor, images, blip_batch_size, device)\n",
    "\n",
    "    # Combining BLIP descriptions into one description\n",
    "    content_user = \"Only state your answer without a period and quotation marks and do not simply repeat the descriptions. State one coherent and concise concept label that is 1-5 words long and can semantically summarize and represent most, not necessarily all, of the conceptual similarities in the following descriptions: \"\n",
    "    for i in range(len(descriptions)):\n",
    "        content_user = content_user + descriptions[i]\n",
    "        if descriptions[i] != descriptions[-1]:\n",
    "            content_user = content_user + ', '\n",
    "    inputs = llama_tokenizer(content_user, return_tensors=\"pt\")\n",
    "    for _ in range(5):\n",
    "        generate_ids = llama_model.generate(inputs.input_ids, max_length=500)\n",
    "        blip_preds = llama_tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "        blip_preds = blip_preds.split(\": \")[-1]\n",
    "        comp_words[orig_id].append(blip_preds)\n",
    "        random.shuffle(descriptions)\n",
    "    \n",
    "    for i, label in enumerate(comp_words[orig_id]):\n",
    "        print(\"Candidate Concept {}: {}\".format(i+1, label))\n",
    "        \n",
    "    all_concepts.loc[len(all_concepts)] = [orig_id] + comp_words[orig_id]\n",
    "\n",
    "# Save candidate concepts\n",
    "utils.save_potential_concepts(all_concepts, results_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af74f2e",
   "metadata": {},
   "source": [
    "### Best Concept Selection (DnD Step 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3274e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We adjust concepts with certain vague words to help SD generation\n",
    "\"\"\"\n",
    "\n",
    "# Limit candidate concept lengths\n",
    "for i in comp_words:\n",
    "    for j in range(len(comp_words[i])):\n",
    "        if len(comp_words[i][j]) > 75:\n",
    "            comp_words[i][j] = comp_words[i][j][:75]\n",
    "            \n",
    "replace_set = ['design','designs','graphic','graphics']\n",
    "for orig_id in ids_to_check:\n",
    "    comp_words[orig_id] = [concept.lower() for concept in comp_words[orig_id]]\n",
    "    for i, word in enumerate(comp_words[orig_id]):\n",
    "        if word[-1] == '.':\n",
    "            comp_words[orig_id][i] = word[:-1]\n",
    "        if word.split()[-1] in replace_set:\n",
    "            new_concept = word + ' background'\n",
    "            comp_words[orig_id].append(new_concept)\n",
    "    comp_words[orig_id] = list(set(comp_words[orig_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2605e0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pil_data = data_utils.get_data(d_probe)\n",
    "d_probe_len = len(pil_data)\n",
    "all_final_results = {neuron_id : [] for neuron_id in ids_to_check}\n",
    "\n",
    "num_images_per_prompt = 10\n",
    "top_K_param = 10\n",
    "beta_images_param = 5\n",
    "scoring_func = 'topk-sq-mean'\n",
    "\n",
    "sd_prompt = 'One realistic image of {}'\n",
    "num_inference_steps = 50\n",
    "\n",
    "# Step 3 Main Code\n",
    "for list_id, orig_id in enumerate(ids_to_check):\n",
    "\n",
    "    # Initialize starting concepts\n",
    "    word_list = comp_words[orig_id]\n",
    "    \n",
    "    print(\"Neuron {}\".format(orig_id))\n",
    "\n",
    "    # Account for modified candidate concept (if necessary)\n",
    "    labels_to_check = len(word_list)\n",
    "\n",
    "    print(\"# Labels to Check: {}\".format(labels_to_check), \"   # Images per Concept: {}\".format(num_images_per_prompt))\n",
    "\n",
    "    add_im = {}\n",
    "    add_im_id = {}\n",
    "    all_sd_imgs = []\n",
    "\n",
    "    # Generate images for each label\n",
    "    for label_id in range(labels_to_check):\n",
    "        \n",
    "        print(\"Label {}/{}: {}\".format(label_id + 1, labels_to_check, word_list[label_id]))\n",
    "        pred_label = sd_prompt.format(word_list[label_id])\n",
    "        add_im_id[label_id] = []\n",
    "        \n",
    "        add_im, add_im_id, all_sd_imgs = DnD_models.generate_sd_images(add_im, add_im_id, all_sd_imgs, \n",
    "                                                                  pred_label, label_id, pipe, generator,\n",
    "                                                                  num_images_per_prompt, num_inference_steps)\n",
    "    \n",
    "    # Concept Scoring\n",
    "    target_feats = utils.get_target_activations(target_name, all_sd_imgs, [target_layer])\n",
    "    \n",
    "    ranks, highest_activating = utils.rank_images(target_feats, orig_id, labels_to_check,\n",
    "                                                 add_im_id, add_im, top_K_param)\n",
    "    \n",
    "    clip_weight = scoring_function.compare_images(top_images[orig_id], highest_activating, clip_name, \n",
    "                                                  device, target_name, top_K_param)\n",
    "    \n",
    "    top_avg_topk = scoring_function.get_score(ranks, mode = scoring_func, hyp_param = beta_images_param)\n",
    "    \n",
    "    top_avg_comb = []\n",
    "    for i in range(len(clip_weight)):\n",
    "        concept_rank = len(top_avg_topk) - scoring_function.find_by_last(top_avg_topk, clip_weight[i][1])\n",
    "        weight = clip_weight[i][0]\n",
    "        concept_score = concept_rank * weight\n",
    "        top_avg_comb.append((concept_score, clip_weight[i][1]))\n",
    "        \n",
    "    top_avg_comb.sort(reverse = True)\n",
    "    \n",
    "    # Save results in .csv file\n",
    "    for label_num in range(3):\n",
    "        if(label_num < len(top_avg_comb)):\n",
    "            all_final_results[orig_id] += [word_list[top_avg_comb[label_num][1]]]\n",
    "        else:\n",
    "            all_final_results[orig_id] += [' ']\n",
    "    final_concepts.loc[len(final_concepts)] = [orig_id] + all_final_results[orig_id]\n",
    "    \n",
    "    # Print results\n",
    "    print('------------------------------\\n')\n",
    "    print('Neuron {}:'.format(orig_id))\n",
    "    for k, word in enumerate(all_final_results[orig_id]):\n",
    "        if(word != \" \"):\n",
    "            print(\"Label {}: {}\".format(k + 1, word))\n",
    "        else:\n",
    "            break\n",
    "    print('\\n------------------------------')\n",
    "    \n",
    "utils.save_final_results(final_concepts, results_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-kernel",
   "language": "python",
   "name": "jupyter-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
